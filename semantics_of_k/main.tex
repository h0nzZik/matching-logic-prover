\documentclass[UTF8]{article}
  \usepackage{xcolor}

  % Select what to do with todonotes: 
  % \usepackage[disable]{todonotes} % notes not showed
  \usepackage[draft]{todonotes} % notes showed

  % Select what to do with command \comment:  
  % \newcommand{\comment}[1]
      {} %comment not showed
  \newcommand{\comment}[1]
    {\par {\bfseries \color{blue} #1 \par}} %comment showed
    
  \usepackage{amsmath}
  \usepackage{amssymb}
  
  \usepackage{amsthm}
  
  % Declare a global counter for theorem environments:
  \newcounter{thmcounter}
  
  % Define new theorem styles:
  \theoremstyle{plain}
  \newtheorem{theorem}[thmcounter]{Theorem}
  \newtheorem{corollary}[thmcounter]{Corollary}
  \newtheorem{lemma}[thmcounter]{Lemma}
  \newtheorem{proposition}[thmcounter]{Proposition}
  \theoremstyle{definition}
  \newtheorem{definition}[thmcounter]{Definition}
  \newtheorem{example}[thmcounter]{Example}
  \theoremstyle{remark}
  \newtheorem{remark}[thmcounter]{Remark}
  \newtheorem{notation}[thmcounter]{Notation}
  
  
  % Package for changing fonts in the Verbatim environment:
  \usepackage{fancyvrb}
  
  % Package for URLs:
  \usepackage{hyperref}  
  
  % Define serif fonts for ML theories used in math mode:
  \newcommand{\PA}{\mathsf{PA}}
  \newcommand{\SEQ}{\mathsf{SEQ}}
  \newcommand{\HEAP}{\mathsf{HEAP}}
  \newcommand{\IMP}{\mathsf{IMP}}
  \newcommand{\FIX}{\mathsf{FIX}}
  \newcommand{\LAMBDA}{\mathsf{LAMBDA}}
  \newcommand{\CTXT}{\mathsf{CTXT}}
  \newcommand{\DEF}{\mathsf{DEF}}
  \newcommand{\METALEVEL}{\mathsf{META-LEVEL}}
  
  % Define serif fonts for symbols:
  \newcommand{\impite}{\mathsf{ite}}
  \newcommand{\impwhile}{\mathsf{while}}
  \newcommand{\imptt}{\mathsf{tt}}
  \newcommand{\impff}{\mathsf{ff}}
  \newcommand{\impskip}{\mathsf{skip}}
  \newcommand{\impseq}{\mathsf{seq}}
  \newcommand{\impasgn}{\mathsf{asgn}}
  
  \newcommand{\impmapsto}{\mathsf{mapsto}}
  \newcommand{\impmerge}{\mathsf{merge}}
  
  \newcommand{\up}{\mathsf{\#up}}
  \newcommand{\down}{\mathsf{\#down}}
  \newcommand{\isGround}{\mathsf{isGround}}
  \newcommand{\xfalse}{\mathsf{false}}
  \newcommand{\xtrue}{\mathsf{true}}
  
  
  % Define identity context
  \newcommand{\I}{\mathsf{I}}
  
  % Define the colon ":" that is used in "x:s"
  % with less spacing around.
  \newcommand{\cln}{{:}}
 
  
  % Define ceiling and flooring symbols:
  \usepackage{mathtools}
  \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

  % Package for underlining and strikethrough texts.
  \usepackage[normalem]{ulem}
  
  % Define text over equality
  \usepackage{mathtools}
  \newcommand{\xeq}[1]
    {\stackrel{\mathclap{\normalfont\tiny\mbox{#1}}}{=}}


  % Title and authors
  \title{The Semantics of K}
  \author{Formal Systems Laboratory \\
          University of Illinois}

\begin{document}

\maketitle

\comment{Please feel free to contribute to this report in all ways. You could add new contents, remove redundant ones, refactor and organize the texts, and correct typos.} 

\section{The Kore Language}

\subsection{Preliminary}

Let us recall the basic grammar of matching logic. We have a countable set of \emph{variable identifiers} to construct logic variables, denoted as $N$, by definition. Assume a matching logic signature $(S, \Sigma)$, which contains a nonempty finite (or infinite?) set of \emph{sorts} and a countable set of \emph{symbols}. Each symbol $\sigma \in \Sigma$, which has a unique identifier in $\Sigma$, has a fixed number of arguments, fixed argument sorts, and a fixed result sort. The well-formed patterns of sort $s \in S$ in generated by the next grammar:
\begin{align*}
\varphi_s \Coloneqq\  &x \cln s \quad \text{where $x \in N$} \\
\mid\  &\varphi_s \wedge \varphi_s \\
\mid\  &\neg \varphi_s \\
\mid\  &\exists x \cln s' . \varphi_s \quad \text{where $x \in N$ and $s' \in S$} \\
\mid\  &\sigma(\varphi_{s_1},\dots,\varphi_{s_n}) \quad \text{where $\sigma \in \Sigma$ has $n$ arguments, and \dots}
\end{align*}

\begin{remark}
	The above \emph{is} a \href{https://en.wikipedia.org/wiki/Formal_grammar}{formal grammar}, whose semantics\footnote{Semantics as a formal grammar, not the matching logic semantics.} is well studied and crystal clear. The grammar defines for each sort $s$ exactly one set of well-formed patterns of sort $s$. We are defining a \emph{set}, that is to say, following the above grammar, one is able to (1) distinguish well-formed patterns from ill-formed ones; and (2) decide whether two well-formed patterns are the same pattern or not. The next remark provides an example of (2). 
\end{remark}

\begin{remark}
	Assume $x, y\in N$ and $s \in S$, pattern $x\cln s \wedge y \cln s$ is a well-formed pattern of sort $s$, by definition. It is also \emph{distinct} from pattern $y\cln s \wedge x \cln s$, by definition. The fact that (after introducing equalities as syntactic sugar in the logic) one can use some proof systems of matching logic and establish $$\vdash x\cln s \wedge y \cln s = y\cln s \wedge x \cln s,$$ or use the semantics of matching logic and establish $$\vDash x\cln s \wedge y \cln s = y\cln s \wedge x \cln s,$$
	has \emph{nothing} to do with the formal grammar itself and does not change the fact that $x\cln s \wedge y \cln s$ and $y\cln s \wedge x \cln s$ are two distinct patterns in matching logic. 
\end{remark}

\begin{definition}[Matching Logic Theory]
\label{ML_theory}
A matching logic theory $(S, \Sigma, A)$ is a triple that contains a nonempty finite set of sorts, a finite or countably infinite set of symbols, and a recursive set of axioms. Two theories are \emph{equal} if they have the same set of sorts and symbols, and they deduce the same set of theorems.
\end{definition}

\comment{I am trying to capture the meta-level theory in the next definition.}
\begin{definition}[The Meta-level Theory]
	The meta-level theory, denoted as $U$, is a meta-logic theory together with a lifting operator $\langle \ \cdot \ \rangle$, such that for any theory $T$ and pattern $\varphi$, 
	\begin{equation*}
	T \vdash \varphi \quad \text{iff} \quad U \vdash \langle T \vdash \varphi \rangle.
	\end{equation*}
\end{definition}

The meta-logic needs not to be the matching logic. In fact, it can simply be a rewriting logic or even equational logic, and we can write it down as a Maude theory. We only use Maude's syntax to specify theories in the meta-logic, so those Maude modules (theories) need no to be executable. In fact, we assume every equations are labeled with the attribute \texttt{nonexe}.

\begin{remark}
	Think of the meta-level theory $U$ as \emph{the theory of ASTs of patterns.}
\end{remark}

\begin{Verbatim}
  fth META-LEVEL
    protecting STRING .
    sorts Symbol Pattern PatternList StringList.
    
    subsort String < StringList .
    op nilStringList : -> StringList .
    op _,_ : StringList StringList -> StringList [assoc] .
    
    subsort Pattern < PatternList .
    op nilPatternList : -> PatternList .
    op _,_ : PatternList PatternList -> PatternList [assoc] .
    
    op #symbol : String StringList String -> Symbol .
    
    op #variable : String String -> Pattern .
    op #and : Pattern Pattern -> Pattern .
    op #not : Pattern -> Pattern .
    op #exists : String String Pattern -> Pattern .
    op #application : Symbol PatternList -> Pattern .
    ---- more things to go ----
    ---- well-formed, getSort, isSort, freshvar, substitution, etc. ----
    
    sort Theorem . ---- patterns that can be deducted
    subsort Theorem < Pattern .
    
    ---- just an example of logic axioms
    mb #not(#and(P:Pattern, #not(P:Pattern))) : Theorem . 
  endth
\end{Verbatim}

The next example is (the meta-representation of) the theory of lambda calculus. Again, equations are assumed to have the attribute \texttt{nonexe}. 

\begin{Verbatim}
fmod LAMBDA
  including META-LEVEL .
  
  ---- the following syntactic sugar is just for readability. 

  ops app lambda0 : -> Symbol .
  eq app = #symbol("app", ("Exp", "Exp"), "Exp") .
  eq lambda0 = #symbol("lambda0", ("Exp", "Exp"), "Exp") .
  
  op lambda_._ : String Pattern -> Pattern .
  eq lambda X:VariableId . E:Pattern
   = #exists(X:VariableId, "Exp", 
     #application(lambda0, (#variable(X:VariableId, "Exp"), 
                           E:Pattern)))) .
  op _[_] : Pattern Pattern -> Pattern .
  eq E1:Pattern[E2:Pattern] 
   = #application(app, (E1:Pattern, E2:Pattern))) .
   
  ---- side conditions checker
  
  op isLTerm : Pattern -> Bool .

  eq isLTerm(#variable(X:VariableId, "Exp")) = true .
  
  eq isLTerm(E1:Pattern[E2:Pattern])
   = isLTerm(E1:Pattern) and isLTerm(E2:Pattern) .
   
  eq isLTerm(lambda(X:VariableId, E:Pattern))
   = isLTerm(E:Pattern) .
  
  ---- the (Beta) axiom
  ---- we haven't defined #equal in the meta-logic yet
  ---- nor #substitute
  cmb #equal((lambda X:VariableId . E1:Pattern)[E2:Pattern],
             #substitute(...)) : Theorem
   if isLTerm(E1:Pattern) and isLTerm(E2:Pattern) .
endfm
\end{Verbatim}

\subsection{The Kore Language}

The Kore language simply provides users a friendly way to write theories in the object-level, which will eventually be desugared to the meta-logic. 

\begin{definition}[The Kore Language]
The Kore language is a language to write matching logic theories. The outcomes are called Kore definitions. Kore definitions are mainly served as the interface between a K frontend and a K backend, but a human should be able to read and write Kore definitions of simple theories, too. The Kore language is designed in a way that:
\begin{itemize}
\item Every Kore definition defines exactly one matching logic theory;
\item Every matching logic theory can be defined as a Kore definition;
\item There is no parsing ambiguity.
\item The least amount of inferring is needed;
\item Symbols are decorated with their argument sorts and result sort;
\item There is no polymorphic or overloaded symbols, so every symbol has a unique name (reference). 
\end{itemize}
\end{definition}

\begin{definition}[Frontend]
A K frontend is an artifact that generates Kore definitions.
\end{definition}

\begin{definition}[Backend]
A K backend is an artifact that consumes a Kore definition of a theory $\mathsf{T}$ and does some work. Whatever it does can and should be algorithmically reduced to the task of proving $\mathsf{T} \vdash \varphi$ where $\varphi$ ``encodes'' that work. A K backend should justify its results by generating formal proofs that can be proof-checked by the oracle matching logic proof checker. Examples of K backends include concrete execution engines, symbolic executions engines, matching logic provers, verification tools, etc.
\end{definition}

\paragraph{Some discussions.}
We spent lots of time considering the design of the Kore language, with which we specify matching logic theories, and we decided to go for meta-logic. This paragraph is a summary that serves as a justification of our choice. 

Suppose we allow users to write infinite axioms to specify theories. In that case, there is no need to go for a meta-logic, since one can always enumerate all axioms in Kore definitions, as long as they are recursively enumerable~(r.e.). Theories who do not have a r.e. set of axioms are not considered as of interest of practice. 

However, we cannot afford our definitions to run forever. We look for a finite representation of them. This is when the concept of meta-variables comes in and plays a role. And soon one realizes that we do not only need meta-variables, but also some notations in the meta-level, too, serving as methods that help us finitely represent the infinite sets of axioms we want to specify. One typical such notation is the substitution $\varphi[\psi/x]$, where $\_[\_/\_]$ is the mathematical notation, in the meta-level, for substitution, which is not part of the object-logic. Other examples include side-conditions on meta-variables and context.

We look for a generic way that helps us represent infinite axioms in a finite way. And there is a natural one. Since axioms are r.e. sets, by definition there are semi-decision procedures that define them, and semi-decision procedures have finite representation. Therefore, instead of enumerating all the axioms, one simply write down the corresponding semi-decision procedure. 

In this section, we will propose a meta-logic that serves as \emph{a logic for matching logic and its proof system.} We believe such as meta-logic will result in a direct implementation of Kore and matching logic provers. 
\newline




\section{Object-level and Meta-level}

It is an aspect of life in mathematical logics to distinguish the \emph{object-level} and \emph{meta-level} concepts. In matching logic, we put more emphasize and care on metavariables and their range, that is, the set of patterns that they stand for. It turns out that having metavariables that range over all well-formed patterns will lead us to inconsistency theories immediately. As an example, consider the ($\beta$) axiom in the matching logic theory $\LAMBDA$ of lambda calculus:
\begin{equation*}
(\lambda x . e) [e'] = e[e' / x].
\end{equation*}
If we do not put any restriction on the range of metavariables $e$ and $e'$, we have an inconsistency issue as the following reasoning shows:
\begin{align*}
\bot \xeq{(N)} (\lambda x . \top)[\bot] \xeq{($\beta$)} \top[\bot / x] = \top.
\end{align*}

Therefore, in matching logic, one should explicitly specify the range of metavariables whenever he uses them.

\begin{definition}[Restricted metavariables]
	Let $\varphi$ be a metavariable of sort $s \in S$. The range of $\varphi$ is a set of patterns of sort $s$. We write $\varphi :: R$ if the range of $\varphi$ is $R \subseteq \mathrm{Pattern}_s$.
\end{definition}

\begin{remark}[Metavariables in first-order logic]
In first-order logic, one often uses metavariables in axiom schemata, but the inconsistency issue does not arise. This is because in first-order logic, we do not need to distinguish metavariables for terms from logic variables, thanks to the next (Substitution) rule:
\begin{equation*}
\forall x . \varphi(x) \to \varphi(t).
\end{equation*}
The predicate metavariables are not a problem because there are no object level symbols on top of them. \comment{I don't get the point of predicate metavariables.}
\end{remark}

\paragraph{Variables and metavariables for variables}

For any matching logic theory $\mathsf{T} = (S, \Sigma, A)$, it comes for each sort $s \in S$ a countably infinite set $V_s$ of variables. We use $\mathsf{x:s}, \mathsf{y:s}, \mathsf{z:s}, \dots$ for variables in $V_s$, and omit their sorts when that is clear from the contexts. Different sorts have disjoint sets of variables, so $\mathrm{Var}_s \cap \mathrm{Var}_{s'} = \emptyset$ if $s \neq s'$.



\begin{proposition}
\label{prop:explicitly_universal_quantified_justification}
Let $A$ be a set of axioms and $\bar{A} = \forall A$ be the universal quantification closure of $A$, then for any pattern $\varphi$, $A \vdash \varphi$ iff $\bar{A} \vdash \varphi$.
\end{proposition}

\begin{remark}[Free variables in axioms]
\label{rmk:free_variables_in_axioms_are_universal_quantified}
The free variables appearing in the axioms of a theory can be regarded as implicitly universal quantified, because a theory and its universal quantification closure are equal.
\end{remark}

\begin{example}
  \begin{align*}
  A_1 &= \{{\sf mult(x, 0) = 0}\} \\
  A_2 &= \{{\sf \forall x . mult(x, 0) = 0}\} \\
  A_3 &= \{{\sf \forall y . mult(y, 0) = 0}\} \\
  A_4 &= \{\mathsf{mult}(x, \mathsf{0}) = \mathsf{0}\} \\
  A_5 &= \{\forall x . \mathsf{mult}(x, \mathsf{0}) = \mathsf{0}\} \\
  A_6 &= \{\forall y . \mathsf{mult}(y, \mathsf{0}) = \mathsf{0}\} \\
  A_7 &= \{{\sf mult(x, 0) = 0, mult(y, 0) = 0, mult(z, 0) = 0, \dots}\} \\
  A_8 &= \{{\sf \forall x . mult(x, 0) = 0, \forall y . mult(y, 0) = 0, \forall z . mult(z, 0) = 0, \dots}\}
  \end{align*}
  
  All the eight theories are equal. Theories $A_4, A_5, A_6$ are finite representations of theories $A_7, A_8, A_8$ respectively. 
\end{example}

\begin{remark}
\label{no_need_to_have_metavariables_for_variables}
There is no need to have metavariables for variables in the Kore language, because (1) if they are used as bound variables, then replacing them with any (matching logic) variables will result in the same theories, thanks to alpha-renaming; and (2) if they are used as free variables, then it makes no difference to consider the universal quantification closure of them and we get to the case (1). 
\comment{\sout{Given said that, there are cases when metavariables for variables make sense. In those cases we often want our metavariables to range over all variables of all sorts, in order to make our Kore definitions compact.} No, we do not need metavariables over variables. I was thinking of the definedness symbols. We might want to write only one axiom schema of $\ceil{x}$ instead many $\ceil{x \cln s}_s^{s'}$'s, but we cannot do that unless we allow polymorphic and overloaded symbols in Kore definitions.}
\end{remark}


\paragraph{Patterns and metavariables for patterns}

It is in practice more common to use metavariables that range over all patterns. One typical example is axiom schemata. For example, $\vdash \varphi \to \varphi$ in which $\varphi$ is the metavariable that ranges all well-formed patterns. 

\comment{There has been an argument on whether metavariables for patterns should be sorted or not. Here are some observations. Firstly, since all symbols are decorated and not overloaded, in most cases, the sort of a metavariable for patterns can be inferred from its context. Secondly, the only counterexample against the first point that I can think of is when they appear alone, which is not an interesting case anyway. Thirdly, we do want the least amount of reasoning and inferring in using Kore definitions, so it breaks nothing if not helping things to have metavariables for patterns carrying their sorts.}

\begin{example}
\begin{align*}
A_1 &= \{{\sf merge(h1, h2) = merge(h2, h1)}\} \\
A_2 &= \{{\sf \forall h1 \forall h2 . merge(h1, h2) = merge(h2, h1)}\} \\
A_3 &= \{\mathsf{merge}(\varphi, \psi) = \mathsf{merge}(\psi, \varphi)\}
\end{align*}
All three theories are equal. It is easier to see that fact from a model theoretic point of view, since all theories require that the interpretation of $\mathsf{merge}$ is commutative and nothing more. On the other hand, it is not straightforward to obtain that conclusion from a proof theoretic point of view. For example, to deduce $\mathsf{merge}(\mathsf{list}(\mathsf{one}, \mathsf{cons}(\mathsf{two}, \mathsf{epsilon})), \mathsf{top}) = \mathsf{merge}(\mathsf{top}, \mathsf{list}(\mathsf{one}, \mathsf{cons}(\mathsf{two}, \mathsf{epsilon})))$ needs only one step in $A_3$, but will need a lot more in either $A_1$ or $A_2$, because one cannot simply substitute any patterns for universal quantified variables in matching logic.

\end{example}

\subsection{The module $\METALEVEL$}

The theory of meta-level in matching logic provides a universe of metarepresentation (abstract syntax trees) of object-level patterns. The theory of meta-level helps us specify the range of metavariables and much more. One important observation is that the connection between object-level modules and the meta-level module cannot be defined in the logic itself. More specifically, the operator $\up$ that takes object-level patterns to their metarepresentation in the meta-level module cannot be defined in the logic no matter how. Consider the next simple example where one defines a symbol $\isGround$ that checks whether a pattern is a ground pattern or not, using the meta-level module:
\begin{align*}
&\isGround(\up[X]) = \xfalse \\
&\isGround(\up[\sigma]) = \xtrue \\
&\isGround(\up[\sigma(\varphi_1,\dots,\varphi_n)]) = \\
&\quad \mathsf{andBool}(\isGround(\up[\varphi_1]), \dots, \isGround(\up[\varphi_n])) \\
&\isGround(\up[\varphi \wedge \psi]) = \\
&\quad \mathsf{andBool}(\isGround(\up[\varphi]), \isGround(\up[\psi]))\\
&\dots
\end{align*}

Unfortunately, the first two axioms lead us to inconsistency, as shown in the following:
\begin{align*}
&\isGround(\up[X - X]) = \xfalse \\
&\isGround(\up[\mathsf{zero}]) = \xtrue
\end{align*}

The only way to prevent such inconsistency issue is to prevent equations propagating through $\up$, more specifically, to forbid the next inference rule:
\begin{align*}
\text{From } \varphi = \psi \text{ deduce } \up[\varphi] = \up[\psi],
\end{align*}
which is a strong evidence that $\up$ is not part of the logic. 

We provide the Kore definition of the $\METALEVEL$ module.

\begin{Verbatim}
module META-LEVEL
  import STRING
  
  syntax Sort
  syntax SortList
  syntax Symbol
  syntax Name
  
  syntax Sort ::= #sort(String)
  syntax Name ::= #name(String)
  syntax Symbol ::= #symbol(Name, SortList, Sort)
  syntax SortList ::= #nilSortList()
                    | SortListAsSort(Sort)
                    | #appendSortList(SortList, SortList)
  syntax Pattern
  syntax Pattern ::= #variable(Name, Sort)
                   | #and(Pattern, Pattern)
                   | #not(Pattern)
                   | #exists(Name, Sort, Pattern)
                   | #application(Symbol, PatternList)
  
  syntax PatternList
  syntax PatternList ::= #nil()
                       | #PatternListAsPattern(Pattern)
                       | #append(PatternList, PatternList)
  
  syntax SortSet SymbolSet PatternSet // defined in the usual way
  
  syntax Module
  syntax Module ::= #module(SortSet, SymbolSet, PatternSet)
                       
  syntax Bool ::= #wellFormed(Pattern)
  syntax Sort ::= #getSort(Pattern)
  // more to go
  
  
endmodule
\end{Verbatim}

Using the $\METALEVEL$ module, we can easily define the ranges of metavariables using \emph{side conditions}. The next section illustrates that.

\section{Binders}

In matching logic there is a unified representation of binders. We will be using the theory of lambda calculus $\LAMBDA$ as an example in this section. Recall that the syntax for untyped lambda calculus is
\begin{align*}
\Lambda \Coloneqq V \mid \lambda V . \Lambda \mid \Lambda \ \Lambda
\end{align*}
where $V$ is a countably infinite set of atomic $\lambda$-terms, a.k.a. variables in lambda calculus. The set of all $\lambda$-terms, denoted as $\Lambda$, is the smallest set satisfying the above grammar.

The matching logic theory $\LAMBDA$ has one sort $\mathsf{Exp}$ for lambda expressions. It also has in its signature a binary symbol $\mathsf{lambda}_0$ that builds a $\lambda$-terms, and a binary symbol $\mathsf{app}$ for lambda applications. To mimic the binding behavior of $\lambda$ in lambda calculus, we define syntactic sugar $\lambda x . e = \exists x . \mathsf{lambda}_0(x, e)$  and $e_1e_2 = \mathsf{app}(e_1, e_2)$ in theory $\LAMBDA$. Notice that by defining $\lambda$ as a syntactic sugar using the existential quantifier $\exists x$, we get alpha-renaming for free. The $\beta$-reduction is captured by the next axiom:
\begin{equation*}
(\lambda x . e)e' = e[e'/x] \quad \text{, where $e$ and $e'$ are metavariables for $\lambda$-terms.}
\end{equation*}

Two important observations are made about the ($\beta$) axiom. Firstly, $e$ and $e'$ cannot be replaced by logic variables, because $\lambda$-terms in matching logic are (often) not functional patterns. Secondly, metavariables $e$ and $e'$ cannot range over all patterns of sort $\mathsf{Exp}$, but only those which are (syntactic sugar of) $\lambda$-terms. Allowing $e$ and $e'$ to range over all patterns of $\mathsf{Exp}$ will quickly lead to an inconsistent theory, because of the next contradiction:
\begin{equation*}
\bot 
\xeq{(N)} (\lambda x . \top)[\bot]
\xeq{($\beta$)} \top.
\end{equation*}

Therefore, when defining the lambda calculus, we need a way 

\begin{theorem}[Consistency]
	Consider a theory of a binder $\alpha$, with a sort $S$ and two binary symbols $\alpha_0$ and $\_ \ \_$. Define $\alpha x . e$ as syntactic sugar of $\exists x . \alpha (x, e)$ where $x$ is a variable and $e$ is a pattern. Define $\alpha$-terms be patterns satisfying the next grammar 
	\begin{align*}
      T_\alpha &\Coloneqq V_s \mid \alpha x .T_\alpha \mid T_\alpha T_\alpha.
	\end{align*}
	If a theory contains only axioms of the form $e = e'$ where $e$ and $e'$ are $\alpha$-terms, then the theory is consistent.
\end{theorem}
\begin{proof}
	The final model $M$ exists, in which the carrier set is a singleton set, and the two symbols are interpreted as the total function over the singleton set. One can then prove that all $\alpha$-terms interpret to the total set, so all axioms hold in the final model.
\end{proof}

\begin{corollary}
	The theory $\LAMBDA$ is consistent.
\end{corollary}


\begin{definition}[Common ranges of metavariables]
\quad
\begin{itemize}
\item Full range $\mathrm{Pattern}_s$;
\item Syntactic terms range (variables plus symbols without logic connectives);
\item Ground syntactic terms range (symbols only); 
\item Variable range $\mathrm{Var}_s$ (metavariables for variables).
\end{itemize}
\end{definition}

\begin{remark}
Syntactic terms (and ground syntactic terms) are purely defined syntactically and not equal to terms or functional patterns. When all symbols are functional symbols, the set of syntactic terms equals the set of terms, and both of them are included in the set of all functional patterns. 
\end{remark}

\begin{remark}
We need to design a syntax for specifying ranges of metavariables in the Kore language. 
\end{remark}

\begin{remark}
We have not proved that matching logic is a conservative extension of untyped lambda calculus, which bothers me a lot. I will remain skeptical about everything we do in this section until we prove that conservative extension result. 
\end{remark}

\comment{The benefit of such a unified theory of binders and binding structures in matching logic is more of theoretical interest. In practice (K backends), one will never want to implement the lambda calculus by desugaring $\lambda x . e$ as $\exists x . \lambda_0(x, \varphi)$ but rather dealing with $\lambda x . \varphi$ directly. }

\begin{example}[Lambda calculus in Kore]
\quad
\begin{Verbatim}[fontsize=\small]
module LAMBDA
  import BOOL
  import META-LEVEL
  
  syntax Exp
  syntax Exp ::= app(Exp, Exp)
               | lambda0(Exp, Exp)
  
  axiom \implies(true = andBool(#isLTerm(#up(E:Exp)),
                                #isLTerm(#up(E':Exp))), 
    app(\exists(x:Exp, lambda0(x:Exp, E:Exp)), E':Exp)
      = E:Exp(E':Exp / x:Exp)
  
  // Q1: what is substitution?
  // Q2: we know #up is not a part of the logic, so what does
  //     it mean?
  
  syntax Bool ::= #isLTerm(Pattern)
  axiom #isLTerm(#variable(x:Name, s:Sort)) = true
  axiom #isLTerm(#application(
    #symbol(#name("app"), #appendSortList(...), #sort("Exp")))),
    #appendPatternList(#PatternListAsPattern(#P),
                       #PatternListAsPattern(#P')))))
  = andBool(#isLTerm(#P), #isLTerm(#P'))
  ...
endmodule
\end{Verbatim}
\end{example}

\todo[inline]{Rewriting logic}

\section{Contexts}

Introduce a binder $\gamma$ together with its application symbol which we write as~$\_[\_]$. Binding variables of the binder $\gamma$ are often written as $\square$, but in this proposal and hopefully in future work we will use regular variables $x, y, z, \dots$ instead of $\square$, in order to show that there is nothing special about contexts but simply a theory in matching logic. Patterns of the form $\gamma x . \varphi$ are often called \emph{contexts}, denoted by metavariables $C, C_0, C_1, \dots$. Patterns of the form $\varphi[\psi]$ are often called \emph{applications}. 
\begin{definition}
	The context $\gamma x . x$ is called the identity context, denoted as $\I$. Identity context has the axiom schema $\I[\varphi]=\varphi$ where $\varphi$ is any pattern.
\end{definition}

\begin{example}
	$\I[\I] = \I$.
\end{example}

\begin{definition}
	Let $\sigma \in \Sigma_{s_1\dots s_n, s}$ is an $n$-arity symbol. We say $\sigma$ is \emph{active} on its $i$th argument ($1 \le i \le n$), if
	$$\sigma(\varphi_1,\dots, C[\varphi_i], \dots, \varphi_n)
	= (\gamma x . \sigma(\varphi_1,\dots,C[x], \dots, \varphi_n))[\varphi_i],$$
	where $\varphi_1,\dots,\varphi_n$, and $C$ are any patterns. Orienting the equation from the left to the right is often called \emph{heating}, while orienting it from the right to the left is called \emph{cooling}.
\end{definition}

\begin{example} Assume the next theory of $\IMP$.
\label{example:IMP}

\begin{align*}
  A = \{&\impite(C[\varphi], \psi_1, \psi_2) = (\gamma x . \impite(C[x], \psi_1, \psi_2))[\varphi], \\
        &\impwhile(C[\varphi], \psi) = (\gamma x . \impwhile(C[x], \psi))[\varphi], \\
        &\impseq(C[\varphi], \psi) = (\gamma x . \impseq(C[x], \psi))[\varphi], \\
        &C[\impite(\imptt, \psi_1, \psi_2)] \Rightarrow C[\psi_1], \\
        &C[\impite(\impff, \psi_1, \psi_2)] \Rightarrow C[\psi_2], \\
        &C[\impwhile(\varphi, \psi)] \Rightarrow C[\impite(\varphi, \impseq(\psi, \impwhile(\varphi, \psi)), \impskip)], \\
        &C[\impseq(\impskip,\psi)] \Rightarrow C[\psi] \}.
\end{align*}

\comment{We can simply require that $\psi_1, \psi_2, \psi_3$, and $C$ are any patterns. That will allow us to do any reasoning that we need, but will that lead to inconsistency?}

\paragraph{Example \ref{example:IMP}(a).} 
	\begin{align*}
	\impseq(\impskip,\impskip)
	&= \mathsf{I}[\impseq(\impskip,\impskip)] \\
	&\Rightarrow \mathsf{I}[\impskip] \\
	&= \impskip.
	\end{align*}
	
\paragraph{Example \ref{example:IMP}(b).} 
	\begin{align*}
	\impseq(\impite(\imptt, \psi_1, \psi_2), \psi_3) 
	&= \impseq(\I[\impite(\imptt, \psi_1, \psi_2)], \psi_3) \\
	&= (\gamma x . \impseq(\I[x], \psi_3))[\impite(\imptt, \psi_1, \psi_2)] \\
	&\Rightarrow (\gamma x . \impseq(\I[x], \psi_3))[\psi_1] \\
	&= \impseq(\I[\psi_1], \psi_3) \\
	&= \impseq(\psi_1, \psi_3).
	\end{align*}
\end{example}

\begin{example}
Consider the following theory written in the Kore language:
\begin{Verbatim}
module IMP
  import ...
  syntax AExp
  syntax AExp ::= plusAExp(AExp, AExp)
  syntax AExp ::= minusAExp(AExp, AExp)
  syntax AExp ::= AExpAsNat(Nat)
  syntax BExp
  syntax BExp ::= geBExp(AExp, AExp)
  syntax BExp ::= BExpAsBool(Bool)
  syntax Pgm
  syntax Pgm  ::= skip()
  syntax Pgm  ::= seq(Pgm Pgm)
  syntax Pgm  ::= 
  syntax Heap
  syntax Cfg
  
endmodule
\end{Verbatim}

\end{example}


\begin{example}
Following the above example, extend $A$ with the next axioms:
\begin{align*}
  \{&C[x][\impmapsto(x, v)] \Rightarrow C[v][\impmapsto(x, v)], \\
    &C[\impasgn(x, v)][\impmapsto(x, v')] \Rightarrow C[\impskip][\impmapsto(x,v)], \\
    &C[\impasgn(x, v)][\varphi] \Rightarrow C[\impskip][\impmerge(\varphi, \impmapsto(x, v))]\}
\end{align*}
\end{example}
\comment{The above example is meant to show the loopup rule, but it does not work because the third axiom is incorrect. Instead of simply writing $\varphi$, we should say that $\varphi$ does not assign any value to $x$. One solution (that is used in the current K backend) is to introduce a strategy language and to extend theories with strategies. }

\begin{example}
	Suppose $f$ and $g$ are binary symbols who are active on their first argument. Suppose $a, b$ are constants, and $x$ is a variable. Let $\square_1$ and $\square_2$ be two hole variables. Define two contexts $C_1 = \gamma \square_1 . f(\square_1, a)$ and $C_2 = \gamma \square_2 . g(\square_2, b)$. 
	
	Because $f$ is active on the first argument, 
	\begin{align*}
       C_1[\varphi]
       &= (\gamma \square_1 . f(\square_1, a)) [\varphi] \\
       &= (\gamma \square_1 . f(\I[\square_1], a)) [\varphi] \\
       &= f(\I[\varphi], a) \\
       &= f(\varphi, a), \text{ for any pattern $\varphi$.}
	\end{align*}
	And for the same reason, $C_2[\varphi] = g(\varphi, b)$. Then we have
	\begin{align*}
	C_1[C_2[x]]
	&= C_1[f(x,a)] \\
	&= g(f(x,a), b).
	\end{align*}
	On the other hand, 
	\begin{align*}
    g(f(x,a), b)
    &= g(C_1[x], b) \\
    &= (\gamma \square . g(C_1[\square], b))[x] \\
    &= (\gamma \square . g(f(\square, a), b))[x].
	\end{align*}
	Therefore, the context $\gamma \square . g(f(\square, a), b))$ is often called the \emph{composition} of $C_1$ and $C_2$, denoted as $C_1 \circ C_2$.

\end{example}

\begin{example}
	Suppose $f$ is a binary symbol with all its two arguments active. Suppose $C_1$ and $C_2$ are two contexts and $a, b$ are constants. Then easily we get
	\begin{align*}
	f(C_1[a],C_2[b])
	&= (\gamma \square_2 . f(C_1[a], C_2[\square_2]))[b] \\
	&= (\gamma \square_2 . ((\gamma \square_1 . f(C_1[\square_1], C_2[\square_2])) [a] )) [b].
	\end{align*}
	What happens above is similar to \emph{curring} a function that takes two arguments. It says that there exists a context $C_a$, related with $C_1, C_2, f$ and $a$ of course, such that $C_a[b]$ returns $f(C_1[a],C_2[b])$. The context $C_a$ has a binding hole $\square_2$, and a body that itself is another context $C_a'$ applied to $a$. In other words, there exists $C_a$ and $C_a'$ such that 
	\begin{itemize}
	\item $f(C_1[a],C_2[b]) = C_a[b]$,
	\item $C_a = \gamma \square_2 . (C_a'[a])$,
	\item $C_a' = \gamma \square_1 . f(C_1[\square_1], C_2[\square_2])$.
	\end{itemize}
	
	A natural question is whether there is a context $C$ such that $C[a][b] = f(C_1[a],C_2[b])$. 
\end{example}

\begin{proposition}
	$C_1[C_2[\varphi]] = C [\varphi], \text{ where } C = \gamma \square . C_1[C_2[\square]].$
\end{proposition}

\subsubsection{Normal forms}

In this section, we consider \emph{decomposition} of patterns. A decomposition of a pattern $P$ is a pair $\langle C, R \rangle $ such that $C[R] = P$. Let us now consider patterns that do not have logical connectives.

\todo[inline]{Fixed points}

\section{The Kore language}

The next grammar is the firstly-proposed Kore language at \href{https://github.com/kframework/kore/wiki/KORE-Language-Syntax}{\underline{here}}.

\begin{Verbatim}[fontsize=\small]

Definition = Attributes
Set{Module}

Module = module ModuleName
Set{Sentence}
endmodule
Attributes

Sentence =  import ModuleName Attributes
| syntax Sort Attributes                           // sort declarations
| syntax Sort ::= Symbol(List{Sort}) Attributes    // symbol declarations
| rule Pattern Attributes
| axiom Pattern Attributes

Attributes = [ List{Pattern} ]

Pattern =  Variable
| Symbol(List{Pattern})                             // symbol applications
| Symbol(Value)                                     // domain values
| \top()
| \bottom()
| \and(Pattern, Pattern)
| \or(Pattern, Pattern)
| \not(Pattern)
| \implies(Pattern, Pattern)
| \exists(Variable, Pattern)
| \forall(Variable, Pattern)
| \next(Pattern)
| \rewrite(Pattern, Pattern)
| \equals(Pattern, Pattern)

Variable =  Name:Sort                                        // variables

ModuleName = RegEx1
Sort       = RegEx2
Name       = RegEx2
Symbol     = RegEx2
Value      = RegEx3

RegEx1 == [A-Z][A-Z0-9-]*
RegEx2 == [a-zA-Z0-9.@#$%^_-]+ | ` [^`]* `
RegEx3 == <Strings>   // Java-style string literals, enclosed in quotes

\end{Verbatim}
In the grammar above, {\small\verb|List{X}|} is a special non-terminal corresponding to possibly empty comma-separated lists of {\small\verb|X|} words (trivial to define in any syntax formalism). {\small\verb|Set{X}|}, on the other hand, is a special non-terminal corresponding to possibly empty space-separated sets of X words. Syntactically, there is no difference between the two (except for the separator), but Kore tools may choose to implement them differently.

\subsection{Builtin theories}

\begin{Verbatim}
module BOOL
syntax Bool
syntax Bool ::= true | false | notBool(Bool)
| andBool(Bool, Bool) | orBool(Bool, Bool)

// axioms for functional symbols
axiom \exists(T:Bool, \equals(T:Bool, true))
axiom \exists(T:Bool, \equals(T:Bool, false))
axiom \exists(T:Bool, \equals(T:Bool, \notBool(X:Bool)))
axiom \exists(T:Bool, \equals(T:Bool, andBool(X:Bool, Y:Bool)))
axiom \exists(T:Bool, \equals(T:Bool, orBool(X:Bool, Y:Bool)))

// axioms for commutativity
axiom \equals(andBool(X:Bool, Y:Bool), andBool(Y:Bool, X:Bool))
axiom \equals(orBool(X:Bool, Y:Bool), orBool(Y:Bool, X:Bool))

// the no-junk axiom for constructors
axiom \or(true, false)

axiom \equals(notBool(true), false)
axiom \equals(notBool(false), true)
axiom \equals(andBool(true, T:Bool), T:Bool)
axiom \equals(andBool(false, T:Bool), false)
axiom \equals(orBool(true, T:Bool), true)
axiom \equals(orBool(false, T:Bool), T:Bool)
endmodule
\end{Verbatim}

\begin{Verbatim}
module META-LEVEL
syntax 
endmodule
\end{Verbatim}

\begin{Verbatim}
module LAMBDA
syntax Exp
syntax Exp ::= lambda0(Exp, Exp) | app(Exp, Exp)

endmodule
\end{Verbatim}



\end{document}