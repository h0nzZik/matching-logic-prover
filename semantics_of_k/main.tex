\documentclass{article}
  \usepackage{xcolor}

  % Select what to do with todonotes: 
  % \usepackage[disable]{todonotes} % notes not showed
  \usepackage[draft]{todonotes} % notes showed

  % Select what to do with command \comment:  
  % \newcommand{\comment}[1]
      {} %comment not showed
  \newcommand{\comment}[1]
    {\par {\bfseries \color{blue} #1 \par}} %comment showed
    
  \usepackage{amsmath}
  \usepackage{amssymb}
  
  \usepackage{amsthm}
  
  % Declare a global counter for theorem environments:
  \newcounter{thmcounter}
  
  % Define new theorem styles:
  \theoremstyle{plain}
  \newtheorem{theorem}[thmcounter]{Theorem}
  \newtheorem{lemma}[thmcounter]{Lemma}
  \newtheorem{proposition}[thmcounter]{Proposition}
  \theoremstyle{definition}
  \newtheorem{definition}[thmcounter]{Definition}
  \newtheorem{example}[thmcounter]{Example}
  \theoremstyle{remark}
  \newtheorem{remark}[thmcounter]{Remark}
  \newtheorem{notation}[thmcounter]{Notation}
  
  % Package for changing fonts in the Verbatim environment:
  \usepackage{fancyvrb}
  
  % Package for URLs:
  \usepackage{hyperref}  
  
  % Define serif fonts for ML theories used in math mode:
  \newcommand{\PA}{\mathsf{PA}}
  \newcommand{\SEQ}{\mathsf{SEQ}}
  \newcommand{\HEAP}{\mathsf{HEAP}}
  \newcommand{\IMP}{\mathsf{IMP}}
  \newcommand{\FIX}{\mathsf{FIX}}
  \newcommand{\LAMBDA}{\mathsf{LAMBDA}}
  \newcommand{\CTXT}{\mathsf{CTXT}}
  \newcommand{\DEF}{\mathsf{DEF}}
  
  % Define serif fonts for IMP program constructors:
  \newcommand{\impite}{\mathsf{ite}}
  \newcommand{\impwhile}{\mathsf{while}}
  \newcommand{\imptt}{\mathsf{tt}}
  \newcommand{\impff}{\mathsf{ff}}
  \newcommand{\impskip}{\mathsf{skip}}
  \newcommand{\impseq}{\mathsf{seq}}
  \newcommand{\impasgn}{\mathsf{asgn}}
  
  \newcommand{\impmapsto}{\mathsf{mapsto}}
  \newcommand{\impmerge}{\mathsf{merge}}
  
  
  % Define identity context
  \newcommand{\I}{\mathsf{I}}
  
  % Define the colon ":" that is used in "x:s"
  % with less spacing around.
  \newcommand{\cln}{{:}}
  
  % Define ceiling and flooring symbols:
  \usepackage{mathtools}
  \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

  % Package for underlining and strikethrough texts.
  \usepackage[normalem]{ulem}
  
  % Define text over equality
  \usepackage{mathtools}
  \newcommand\xeq[1]
    {\stackrel{\mathclap{\normalfont\tiny\mbox{#1}}}{=}}


  % Title and authors
  \title{The Semantics of K}
  \author{Formal Systems Laboratory \\
          University of Illinois}

\begin{document}

\maketitle

\comment{Please feel free to contribute to this report in all ways. You could add new contents, remove redundant ones, refactor and organize the texts, and correct typos.} 

\begin{definition}[Matching logic theory]
\label{ML_theory}
A matching logic theory $(S, \Sigma, A)$ is a triple that contains a nonempty finite set of sorts, a finite or countably infinite set of symbols, and a recursive set of axioms. Two theories are \emph{equal} if they have the same set of sorts and symbols, and they deduce the same set of theorems.
\end{definition}

\begin{example}
We use {\sf serif fonts} to denote matching logic theories. Some of the commonly used ones are the theory (theories) of definedness $\DEF$, the theory of Presburger arithmetic $\PA$, the theory of sequences of natural numbers $\SEQ$, the theory of memory heaps $\HEAP$, the theory of {IMP} programs $\IMP$, the theory (theories) of fixed-points $\FIX$, and the theory (theories) of contexts $\CTXT$.
\end{example}


\begin{definition}[The Kore Language]
The Kore language is a language to write matching logic theories. The outcomes are called Kore definitions. Kore definitions are mainly served as the interface between a K frontend and a K backend, but a human should be able to read and write Kore definitions of simple theories, too. The Kore language is designed in a way that:
\begin{itemize}
\item Every Kore definition defines exactly one matching logic theory;
\item Every matching logic theory can be defined as a Kore definition;
\item There is no parsing ambiguity.
\item The least amount of inferring is needed;
\item Symbols are decorated with their argument sorts and result sort;
\item There is no polymorphic or overloaded symbols, so every symbol has a unique name (reference). 
\comment{The above two points will lead to some redundancy in Kore definitions, though. For example, there will be $n^2$ definedness symbol if there are $n$ sorts in the theory. Similarly, $n^2$ equalities are need, too.}
\item User-defined (matching logic) variables are allowed as in theory $\LAMBDA$. We shouldn't fix a syntax for variables. Please refer to later sections about theory $\LAMBDA$ for more discussions.
\item And more \dots
\end{itemize}

The next grammar is the firstly-proposed Kore language at \href{https://github.com/kframework/kore/wiki/KORE-Language-Syntax}{\underline{here}}.
\begin{Verbatim}[fontsize=\small]

Definition = Attributes
             Set{Module}

Module = module ModuleName
           Set{Sentence}
         endmodule
         Attributes

Sentence =  import ModuleName Attributes
          | syntax Sort Attributes                           // sort declarations
          | syntax Sort ::= Symbol(List{Sort}) Attributes    // symbol declarations
          | rule Pattern Attributes
          | axiom Pattern Attributes

Attributes = [ List{Pattern} ]

Pattern =  Variable
         | Symbol(List{Pattern})                             // symbol applications
         | Symbol(Value)                                     // domain values
         | \top()
         | \bottom()
         | \and(Pattern, Pattern)
         | \or(Pattern, Pattern)
         | \not(Pattern)
         | \implies(Pattern, Pattern)
         | \exists(Variable, Pattern)
         | \forall(Variable, Pattern)
         | \next(Pattern)
         | \rewrite(Pattern, Pattern)
         | \equals(Pattern, Pattern)

Variable =  Name:Sort                                        // variables

ModuleName = RegEx1
Sort       = RegEx2
Name       = RegEx2
Symbol     = RegEx2
Value      = RegEx3

RegEx1 == [A-Z][A-Z0-9-]*
RegEx2 == [a-zA-Z0-9.@#$%^_-]+ | ` [^`]* `
RegEx3 == <Strings>   // Java-style string literals, enclosed in quotes

\end{Verbatim}

In the grammar above, List{X} is a special non-terminal corresponding to possibly empty comma-separated lists of X words (trivial to define in any syntax formalism). Set{X}, on the other hand, is a special non-terminal corresponding to possibly empty space-separated sets of X words. Syntactically, there is no difference between the two (except for the separator), but KORE tools may choose to implement them differently.

\end{definition}

\begin{definition}[Frontend]
A K frontend is an artifact that generates Kore definitions.
\end{definition}

\begin{definition}[Backend]
A K backend is an artifact that consumes a Kore definition of a theory $\mathsf{T}$ and does some work. Whatever it does can and should be algorithmically reduced to the task of proving $\mathsf{T} \vdash \varphi$ where $\varphi$ ``encodes'' that work. A K backend should justify its results by generating formal proofs that can be proof-checked by the oracle matching logic proof checker.
\end{definition}

\section{Object-level and meta-level}

It is an aspect of life in mathematical logics to distinguish the \emph{object-level} and \emph{meta-level} concepts. The basic principle is to use serif fonts letters ($\mathsf{x}$) for object-level concepts and normal math fonts letters for meta-level concepts ($x$). 

\paragraph{Variables and metavariables for variables}

For any matching logic theory $\mathsf{T} = (S, \Sigma, A)$, it comes for each sort $s \in S$ a countably infinite set $V_s$ of variables. We use $\mathsf{x:s}, \mathsf{y:s}, \mathsf{z:s}, \dots$ for variables in $V_s$, and omit their sorts when that is clear from the contexts. Different sorts have disjoint sets of variables, so $\mathrm{Var}_s \cap \mathrm{Var}_{s'} = \emptyset$ if $s \neq s'$.



\begin{proposition}
\label{prop:explicitly_universal_quantified_justification}
Let $A$ be a set of axioms and $\bar{A} = \forall A$ be the universal quantification closure of $A$, then for any pattern $\varphi$, $A \vdash \varphi$ iff $\bar{A} \vdash \varphi$.
\end{proposition}

\begin{remark}[Free variables in axioms]
\label{rmk:free_variables_in_axioms_are_universal_quantified}
The free variables appearing in the axioms of a theory can be regarded as implicitly universal quantified, because a theory and its universal quantification closure are equal.
\end{remark}

\begin{example}
  \begin{align*}
  A_1 &= \{{\sf mult(x, 0) = 0}\} \\
  A_2 &= \{{\sf \forall x . mult(x, 0) = 0}\} \\
  A_3 &= \{{\sf \forall y . mult(y, 0) = 0}\} \\
  A_4 &= \{\mathsf{mult}(x, \mathsf{0}) = \mathsf{0}\} \\
  A_5 &= \{\forall x . \mathsf{mult}(x, \mathsf{0}) = \mathsf{0}\} \\
  A_6 &= \{\forall y . \mathsf{mult}(y, \mathsf{0}) = \mathsf{0}\} \\
  A_7 &= \{{\sf mult(x, 0) = 0, mult(y, 0) = 0, mult(z, 0) = 0, \dots}\} \\
  A_8 &= \{{\sf \forall x . mult(x, 0) = 0, \forall y . mult(y, 0) = 0, \forall z . mult(z, 0) = 0, \dots}\}
  \end{align*}
  
  All the eight theories are equal. Theories $A_4, A_5, A_6$ are finite representations of theories $A_7, A_8, A_8$ respectively. 
\end{example}

\begin{remark}
\label{no_need_to_have_metavariables_for_variables}
There is no need to have metavariables for variables in the Kore language, because (1) if they are used as bound variables, then replacing them with any (matching logic) variables will result in the same theories, thanks to alpha-renaming; and (2) if they are used as free variables, then it makes no difference to consider the universal quantification closure of them and we get to the case (1). 
\comment{\sout{Given said that, there are cases when metavariables for variables make sense. In those cases we often want our metavariables to range over all variables of all sorts, in order to make our Kore definitions compact.} No, we do not need metavariables over variables. I was thinking of the definedness symbols. We might want to write only one axiom schema of $\ceil{x}$ instead many $\ceil{x \cln s}_s^{s'}$'s, but we cannot do that unless we allow polymorphic and overloaded symbols in Kore definitions.}
\end{remark}

\paragraph{Patterns and metavariables for patterns}

It is in practice more common to use metavariables that range over all patterns. One typical example is axiom schemata. For example, $\vdash \varphi \to \varphi$ in which $\varphi$ is the metavariable that ranges all well-formed patterns. 

\comment{There has been an argument on whether metavariables for patterns should be sorted or not. Here are some observations. Firstly, since all symbols are decorated and not overloaded, in most cases, the sort of a metavariable for patterns can be inferred from its context. Secondly, the only counterexample against the first point that I can think of is when they appear alone, which is not an interesting case anyway. Thirdly, we do want the least amount of reasoning and inferring in using Kore definitions, so it breaks nothing if not helping things to have metavariables for patterns carrying their sorts.}

\begin{example}
\begin{align*}
A_1 &= \{{\sf merge(h1, h2) = merge(h2, h1)}\} \\
A_2 &= \{{\sf \forall h1 \forall h2 . merge(h1, h2) = merge(h2, h1)}\} \\
A_3 &= \{\mathsf{merge}(\varphi, \psi) = \mathsf{merge}(\psi, \varphi)\}
\end{align*}
All three theories are equal. It is easier to see that fact from a model theoretic point of view, since all theories require that the interpretation of $\mathsf{merge}$ is commutative and nothing more. On the other hand, it is not straightforward to obtain that conclusion from a proof theoretic point of view. For example, to deduce $\mathsf{merge}(\mathsf{list}(\mathsf{one}, \mathsf{cons}(\mathsf{two}, \mathsf{epsilon})), \mathsf{top}) = \mathsf{merge}(\mathsf{top}, \mathsf{list}(\mathsf{one}, \mathsf{cons}(\mathsf{two}, \mathsf{epsilon})))$ needs only one step in $A_3$, but will need a lot more in either $A_1$ or $A_2$, because one cannot simply substitute any patterns for universal quantified variables in matching logic.
\comment{This is a nice example that shows the power of metavariables and how they greatly shorten formal proofs, but this is not a good example to convince people that we need metavariables for patterns in order to embrace more expressiveness. There is an example in the theory $\LAMBDA$ that clearly shows metavariables give us more expressiveness power, and we will cover that example in later sections.}
\end{example}

\section{Binders}

In matching logic there is a unified representation of binders. We will be using the theory of lambda calculus $\LAMBDA$ as an example in this section. Recall that the syntax for untyped lambda calculus is
\begin{align*}
\Lambda \Coloneqq V \mid \lambda V . \Lambda \mid \Lambda \ \Lambda
\end{align*}
where $V$ is a countably infinite set of atomic $\lambda$-terms. The set of all $\lambda$-terms is the smallest set satisfying the above grammar.

The theory $\LAMBDA$ in matching logic has one sort $\mathsf{Exp}$ for lambda expressions. It also has in its signature a binary symbol $\mathsf{\#lambda}$ that builds a $\lambda$-terms, and a binary symbol $\mathsf{app}$ for lambda applications. To mimic the binding behavior of $\lambda$ in lambda calculus, we define syntactic sugar $\mathsf{lambda}x . e = \exists x . \mathsf{\#lambda}(x, e)$  and $e_1e_2 = \mathsf{app}(e_1, e_2)$ in theory $\LAMBDA$. Notice that by defining $\mathsf{lambda}$ as a syntactic sugar using existential quantifier, we get alpha-renaming for free. The $\beta$-reduction is captured in $\LAMBDA$ by the next ($\beta$) axiom:
\begin{equation*}
(\mathsf{lambda} x . e)e' = e[e'/x] \quad \text{, where $e$ and $e'$ are metavariables for $\lambda$-terms.}
\end{equation*}

We have two important observations about the ($\beta$) axiom. Firstly, metavaraibles $e$ and $e'$ cannot be replaced by matching logic variables, because $\lambda$-terms in matching logic are (often) not functional patterns. Secondly, metavariables $e$ and $e'$ cannot range over all patterns of sort $\mathsf{Exp}$, but only those which are (syntactic sugar) of $\lambda$-terms. Allowing $e$ and $e'$ range over all patterns of $\mathsf{Exp}$ in the ($\beta$) axiom will quickly lead to an inconsistent theory, because the next contradiction becomes an instance of the ($\beta$) axiom.
\begin{equation*}
\bot 
\xeq{(N)} \mathsf{app}((\mathsf{lambda} x . \top), \bot)
\xeq{($\beta$)} \top.
\end{equation*}

\comment{The above example is a strong evidence that we need metavariables that range over only a restricted set of patterns instead of all of them. Such a restricted set of patterns is called the \emph{range} (or maybe \emph{domain}?) of metavariables, which we will formally define later in this section. However, before we get into that, here are some observations that against having such restricted metavariables.
\begin{enumerate}
\item We found no example that has such inconsistency issue except theories that contain binders and applications. Therefore, restricted metavariables, even if we do introduce them in the Kore language, will be used in a quite limited way in defining theories that have binders, such as the lambda calculus and the theory of contexts, and will hardly be used in theories that don't have binders;
\item The reason why we need metavariables is because binding structures, such as $\lambda$-terms, are defined in matching logic as syntactic sugar with existential quantification. This makes those binding structures, such as $\lambda$-terms, no longer matching logic terms, and thus cannot substitute for matching logic variables, while they can (and should) be able to substitute for variables in their original calculus.
\item Introducing restrictive metavariables does solve the inconsistency issue, but it is apparently not the only solution. An alternative solution requires us to reexamine binders and binding structures in matching logic. If we could find a way to encode, say $\lambda$-terms, as \emph{functional patterns} in matching logic, then using variables is sufficient, and we can get rid of restrictive metavariables.
\item Having a unified theory of binders in matching logic is a good idea. Splitting binding structures into the process of constructing a term and the process of create a binding is also a promising way to go. We know how to construct a term in matching logic by simply using symbols. The issue is how to create a binding between variables and terms (or patterns in general).
\item A quantifier, given it a universal or existential one, creates a binding between a variable and a pattern \emph{and does something more}. According the semantics of matching logic, A universal quantifier creates the binding and calculates the big conjunction of all instances while an existential quantifier creates the binding and calculates the big disjunction of them. 
\item On the other hand, most binders that we use simply construct a term and create a binding between a variable and the term, and we should not pour any semantics upon them. However, when we use the existential quantifier to create a binding relation, we do put extra semantics upon those user-defined binders. That is the fundamental reason why $\lambda$-terms, even though they should be terms, become nonfunctional patterns in matching logic as we have seen, and thus we need to design the Kore language to support restricted metavariables.
\item Why not introduce a ``pure'' binder that creates a binding between variables and patterns and does nothing else at all? All other binders such as the $\lambda$ in the lambda calculus, the $\mu$ and $\nu$ in theories of fixed point, the $\gamma$ in theory of contexts, and even the universal quantifier $\forall$ and existential quantifier $\exists$, can all be defined using that ``pure'' binder.
\end{enumerate}
}
\begin{definition}[Restricted metavariables]
Let $\varphi$ be a metavariable of sort $s \in S$. The range of $\varphi$ is a subset of all patterns of the sort $s$. We write $\varphi :: R$ if the range of $\varphi$ is $R \subseteq \mathrm{Pattern}_s$.
\end{definition}

\begin{definition}[Common ranges of metavariables]
\quad
\begin{itemize}
\item Full range $\mathrm{Pattern}_s$;
\item Syntactic terms range (variables plus symbols without logic connectives);
\item Ground syntactic terms range (symbols only); 
\item Variable range $\mathrm{Var}_s$ (metavariables for variables).
\end{itemize}
\end{definition}

\begin{remark}
Syntactic terms (and ground syntactic terms) are purely defined syntactically and not equal to terms or functional patterns. When all symbols are functional symbols, the set of syntactic terms equals the set of terms, and both of them are included in the set of all functional patterns. 
\end{remark}

\begin{remark}
We need to design a syntax for specifying ranges of metavariables in the Kore language. 
\end{remark}

\begin{remark}
We have not proved that matching logic is a conservative extension of untyped lambda calculus, which bothers me a lot. I will remain skeptical about everything we do in this section until we prove that conservative extension result. 
\end{remark}

\comment{The only benefit of trying to have such a unified theory of binders and binding structures is of theoretical interest. In practice, one will never want to implement the lambda calculus by desugaring it as $\exists x . \#lambda(x, \varphi)$ but rather dealing with $\lambda x . \varphi$ directly. }

\begin{example}[Lambda calculus in Kore]
\quad
\begin{Verbatim}[fontsize=\small]
module LAMBDA
  import ID
  syntax Exp
  syntax Exp ::= ExpAsId(Id)
  syntax Exp ::= app(Exp, Exp)
  syntax Exp ::= lambda0(Exp, Exp)
  
  axiom app(\exists(id:Id, lambda0(ExpAsId(id:Id), E:SyntacticTerms(Exp))), 
            E':SyntacticTerms(Exp))
      = E:SyntacticTerms(Exp)(E':SyntacticTerms(Exp) / ExpAsId(id:Id))
  ...
endmodule

module ID
  syntax Id 
endmodule
\end{Verbatim}
\end{example}

\todo[inline]{Rewriting logic}

\section{Contexts}

Introduce a binder $\gamma$ together with its application symbol which we write as~$\_[\_]$. Binding variables of the binder $\gamma$ are often written as $\square$, but in this proposal and hopefully in future work we will use regular variables $x, y, z, \dots$ instead of $\square$, in order to show that there is nothing special about contexts but simply a theory in matching logic. Patterns of the form $\gamma x . \varphi$ are often called \emph{contexts}, denoted by metavariables $C, C_0, C_1, \dots$. Patterns of the form $\varphi[\psi]$ are often called \emph{applications}. 
\begin{definition}
	The context $\gamma x . x$ is called the identity context, denoted as $\I$. Identity context has the axiom schema $\I[\varphi]=\varphi$ where $\varphi$ is any pattern.
\end{definition}

\begin{example}
	$\I[\I] = \I$.
\end{example}

\begin{definition}
	Let $\sigma \in \Sigma_{s_1\dots s_n, s}$ is an $n$-arity symbol. We say $\sigma$ is \emph{active} on its $i$th argument ($1 \le i \le n$), if
	$$\sigma(\varphi_1,\dots, C[\varphi_i], \dots, \varphi_n)
	= (\gamma x . \sigma(\varphi_1,\dots,C[x], \dots, \varphi_n))[\varphi_i],$$
	where $\varphi_1,\dots,\varphi_n$, and $C$ are any patterns. Orienting the equation from the left to the right is often called \emph{heating}, while orienting it from the right to the left is called \emph{cooling}.
\end{definition}

\begin{example} Assume the next theory of $\IMP$.
\label{example:IMP}

\begin{align*}
  A = \{&\impite(C[\varphi], \psi_1, \psi_2) = (\gamma x . \impite(C[x], \psi_1, \psi_2))[\varphi], \\
        &\impwhile(C[\varphi], \psi) = (\gamma x . \impwhile(C[x], \psi))[\varphi], \\
        &\impseq(C[\varphi], \psi) = (\gamma x . \impseq(C[x], \psi))[\varphi], \\
        &C[\impite(\imptt, \psi_1, \psi_2)] \Rightarrow C[\psi_1], \\
        &C[\impite(\impff, \psi_1, \psi_2)] \Rightarrow C[\psi_2], \\
        &C[\impwhile(\varphi, \psi)] \Rightarrow C[\impite(\varphi, \impseq(\psi, \impwhile(\varphi, \psi)), \impskip)], \\
        &C[\impseq(\impskip,\psi)] \Rightarrow C[\psi] \}.
\end{align*}

\comment{We can simply require that $\psi_1, \psi_2, \psi_3$, and $C$ are any patterns. That will allow us to do any reasoning that we need, but will that lead to inconsistency?}

\paragraph{Example \ref{example:IMP}(a).} 
	\begin{align*}
	\impseq(\impskip,\impskip)
	&= \mathsf{I}[\impseq(\impskip,\impskip)] \\
	&\Rightarrow \mathsf{I}[\impskip] \\
	&= \impskip.
	\end{align*}
	
\paragraph{Example \ref{example:IMP}(b).} 
	\begin{align*}
	\impseq(\impite(\imptt, \psi_1, \psi_2), \psi_3) 
	&= \impseq(\I[\impite(\imptt, \psi_1, \psi_2)], \psi_3) \\
	&= (\gamma x . \impseq(\I[x], \psi_3))[\impite(\imptt, \psi_1, \psi_2)] \\
	&\Rightarrow (\gamma x . \impseq(\I[x], \psi_3))[\psi_1] \\
	&= \impseq(\I[\psi_1], \psi_3) \\
	&= \impseq(\psi_1, \psi_3).
	\end{align*}
\end{example}

\begin{example}
Consider the following theory written in the Kore language:
\begin{Verbatim}
module IMP
  import ...
  syntax AExp
  syntax AExp ::= plusAExp(AExp, AExp)
  syntax AExp ::= minusAExp(AExp, AExp)
  syntax AExp ::= AExpAsNat(Nat)
  syntax BExp
  syntax BExp ::= geBExp(AExp, AExp)
  syntax BExp ::= BExpAsBool(Bool)
  syntax Pgm
  syntax Pgm  ::= skip()
  syntax Pgm  ::= seq(Pgm Pgm)
  syntax Pgm  ::= 
  syntax Heap
  syntax Cfg
  
endmodule
\end{Verbatim}

\end{example}


\begin{example}
Following the above example, extend $A$ with the next axioms:
\begin{align*}
  \{&C[x][\impmapsto(x, v)] \Rightarrow C[v][\impmapsto(x, v)], \\
    &C[\impasgn(x, v)][\impmapsto(x, v')] \Rightarrow C[\impskip][\impmapsto(x,v)], \\
    &C[\impasgn(x, v)][\varphi] \Rightarrow C[\impskip][\impmerge(\varphi, \impmapsto(x, v))]\}
\end{align*}
\end{example}
\comment{The above example is meant to show the loopup rule, but it does not work because the third axiom is incorrect. Instead of simply writing $\varphi$, we should say that $\varphi$ does not assign any value to $x$. One solution (that is used in the current K backend) is to introduce a strategy language and to extend theories with strategies. }

\begin{example}
	Suppose $f$ and $g$ are binary symbols who are active on their first argument. Suppose $a, b$ are constants, and $x$ is a variable. Let $\square_1$ and $\square_2$ be two hole variables. Define two contexts $C_1 = \gamma \square_1 . f(\square_1, a)$ and $C_2 = \gamma \square_2 . g(\square_2, b)$. 
	
	Because $f$ is active on the first argument, 
	\begin{align*}
       C_1[\varphi]
       &= (\gamma \square_1 . f(\square_1, a)) [\varphi] \\
       &= (\gamma \square_1 . f(\I[\square_1], a)) [\varphi] \\
       &= f(\I[\varphi], a) \\
       &= f(\varphi, a), \text{ for any pattern $\varphi$.}
	\end{align*}
	And for the same reason, $C_2[\varphi] = g(\varphi, b)$. Then we have
	\begin{align*}
	C_1[C_2[x]]
	&= C_1[f(x,a)] \\
	&= g(f(x,a), b).
	\end{align*}
	On the other hand, 
	\begin{align*}
    g(f(x,a), b)
    &= g(C_1[x], b) \\
    &= (\gamma \square . g(C_1[\square], b))[x] \\
    &= (\gamma \square . g(f(\square, a), b))[x].
	\end{align*}
	Therefore, the context $\gamma \square . g(f(\square, a), b))$ is often called the \emph{composition} of $C_1$ and $C_2$, denoted as $C_1 \circ C_2$.

\end{example}

\begin{example}
	Suppose $f$ is a binary symbol with all its two arguments active. Suppose $C_1$ and $C_2$ are two contexts and $a, b$ are constants. Then easily we get
	\begin{align*}
	f(C_1[a],C_2[b])
	&= (\gamma \square_2 . f(C_1[a], C_2[\square_2]))[b] \\
	&= (\gamma \square_2 . ((\gamma \square_1 . f(C_1[\square_1], C_2[\square_2])) [a] )) [b].
	\end{align*}
	What happens above is similar to \emph{curring} a function that takes two arguments. It says that there exists a context $C_a$, related with $C_1, C_2, f$ and $a$ of course, such that $C_a[b]$ returns $f(C_1[a],C_2[b])$. The context $C_a$ has a binding hole $\square_2$, and a body that itself is another context $C_a'$ applied to $a$. In other words, there exists $C_a$ and $C_a'$ such that 
	\begin{itemize}
	\item $f(C_1[a],C_2[b]) = C_a[b]$,
	\item $C_a = \gamma \square_2 . (C_a'[a])$,
	\item $C_a' = \gamma \square_1 . f(C_1[\square_1], C_2[\square_2])$.
	\end{itemize}
	
	A natural question is whether there is a context $C$ such that $C[a][b] = f(C_1[a],C_2[b])$. 
\end{example}

\begin{proposition}
	$C_1[C_2[\varphi]] = C [\varphi], \text{ where } C = \gamma \square . C_1[C_2[\square]].$
\end{proposition}

\subsubsection{Normal forms}

In this section, we consider \emph{decomposition} of patterns. A decomposition of a pattern $P$ is a pair $\langle C, R \rangle $ such that $C[R] = P$. Let us now consider patterns that do not have logical connectives.

\todo[inline]{Fixed points}



\end{document}